<!DOCTYPE html>
<html>
<head>
	<title>15418 Final Project</title>

	<!--Import Google Icon Font-->
	

	<meta name="viewport" content="width=device-width, initial-scale=1.0"/>

	<script type="text/javascript" src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
    <link href="http://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
  	<link rel="stylesheet" href="css/materialize.min.css">
    <link rel="stylesheet" href="css/style.css">
  	<script src="js/materialize.min.js"></script>
</script>

</head>

<body>
	<nav>
    <div class="nav-wrapper amanda-navbar">
      <a class="brand-logo">&nbsp;&nbsp;15-418 Final Project</a>
      <ul id="nav-mobile" class="right hide-on-med-and-down">
        <li><a href="proposal.html">Proposal</a></li>
        <li><a href="checkpoint.html">Checkpoint</a></li>
        <li><a href="index.html">Final Write Up</a></li>
        <li><a href="http://www.github.com">Code</a></li>
      </ul>
    </div>
  </nav>

  <div id ="fullpage" class="container left-align">
  	<div class="section">
     <h2>Parallel Nearest Neighbor Search for Velodyne LiDAR Point Clouds</h2>
  	 <h3>Final Report</h3>
     <p><a href="https://github.com/haoala">Adriel Luo</a> and <a href="https://github.com/setttings">Amanda Eng</a></p>
     <h5>Summary</h5>
     <p>We implemented a parallel nearest neighbor search that is optimized for point clouds returned by Velodyne LiDAR sensor, which are universally used on autonomous vehicles for sensing, localization and mapping. Nearest neighbor search is a fundamental step in iterative closest point (ICP) algorithms used to perform mapping. Our algorithm is currently able to achieve a 10x speed up over an optimized sequential kdtree-based nearest neighbors library implementation through GPU parallelism and by taking advantage of the 3D structure of Velodyne LiDAR point clouds, but we expect that we will be able to achieve a speed up can be as high as 100x.</p>
     <br>
     <h5>Major technical challenges</h5>
     <p>One of the challenges arises from the fact that we are trying to outperform a sequential kd-tree-based nearest neighbor search implementation that is is fast and optimized (the Fast Library for Approximate Nearest Neighbors or FLANN). This means that we need to minimize the impact of the overhead of using the GPU for computation.</p>
     <p>Parallelizing this algorithm is not trivial due to hardware and memory constraints. Point clouds have more than 120,000 points on average, so a naive pairwise brute force distance computation would need enough memory for 120,000&sup2; = 14 billion entries, or 14 billion threads in total. Therefore we need to be smart about data reuse in order to reduce the memory footprint of the algorithm. We also try to use shared memory where possible to reduce the latency of memory access.</p>
     <br>
     <h5>Preliminary Results</h5>
     <h6><i>Establishing a baseline</i></h6>
     <p>Let the reference dataset refer to the points captured at time t, and the query dataset refer to data obtained at time t+1. For this problem, we intend to find the nearest neighbor in the reference dataset for every point in the query dataset.</p>
     <p>In the table below, the construction phase referred to the time it took to create a point cloud, and the querying phase refers to the total time taken for every point in the query dataset to find its nearest neighbor in the reference dataset. Both datasets contain more than 120,000 points.</p>
     <center><img src="img/img7.png" width="300" height="300" align="middle"></center>
     <br>

     <h6><i>Parallelizing over points</i></h6>
     <p>We developed a CUDA implementation for nearest neighbors that parallelized over the pixels in the reference point cloud and calculated the nearest neighbor for each point in the target point cloud sequentially.  In order to find out if we are maximizing the parallelism available with CUDA, we varied the number of points in the reference point cloud and obtained the following graph. </p>
     <center><img src="img/img3.png" width="600" height="400" align="middle"></center>
     <center><img src="img/img4.png" width="600" height="400" align="middle"></center>
     <p>As we can see from Figures 1 and 2 above, the CUDA times remained around the same (and were actually slower than the sequential implementation)  when the reference dataset contained around 2,000 points. This indicated that the overheads of the CUDA kernel calls dominated the time taken.</p>
     <p>Despite the massive parallelism that using CUDA brought, it was still <b>40.3x slower than the sequential PCL kd-tree implementation</b> with FLANN.  This led us to think of other implementations that involved launching fewer CUDA kernels or preprocessing the data.</p>
     <br>

     <h6><i>Parallelizing over points in batches</i></h6>
     <p>We moved on to use parallelize over points in batches of size <i>chunksize</i>.  This meant that the work performed by each CUDA thread increased by a factor of chunksize, but we made less CUDA calls by a factor of chunksize.  This turned out to be <b>4.3x faster our prior parallel CUDA algorithm and 10.17x slower than the sequential PCL kd-tree implementation</b>.  This speedup can possibly be explained by the overhead of launching the additional CUDA kernels. </p>
     <br>
     <h6><i>Reducing search space</i></h6>
     <center><img src="img/img6.png" width="500" height="300" align="middle"></center>
     <p>Reducing the search space for nearest neighbors gives us significant additional speed up. Taking advantage of the 3D structure of Velodyne point clouds, as well as the most common use case, we can partition the point clouds by azimuth, and only search for nearest neighbors in the partition with similar azimuth. We sacrifice some correctness in reducing the search space, because the correctness nearest neighbor might not be within the space we are searching, but because this is an optimization problem and the data has noise to begin with, having some outliers is acceptable.</p>

     <center><img src="img/img5.png" width="600" height="350" align="center"></center>

     <p>We see that correctness is maintained better for nearest neighbor searches between consecutive frames than between non-consecutive frames. This is not surprising, since consecutive frames, with a temporal difference of only 0.1s, are structurally similar and the simplifying assumptions we make above about where to find the nearest neighbor are more valid. Indeed, searching for nearest neighbors between consecutive frames is the most common use case, because in lidar mapping applications, the map is incrementally built every frame, and every new incoming frame is matched with the prior frame.</p>

     <p>The horizontal axis, speedup, is proportional to the number of radial partitions. We can sacrifice some correctness to achieve a significant speed up. For instance, <b>with consecutive frames, we can achieve a speedup of 48x and still find the correct nearest neighbor 99.7% of the time.</b> We can achieve <b>correctness scores of 98.2% and 94.9% with a speedup of 100x and 200x respectively</b>.</p>

     <p>Depending on the level of correctness needed by an application and whether it is finding nearest neighbors in consecutive or non-consecutive frames, one can choose the maximum amount of speedup that meets the constraints.</p>
     <br>

     <h5>Summary of what final numbers on Friday are expected to show</h5>
     <p>We expect that, once we are able to parallelize over points in the reference and target point clouds, we will be able to exceed the 10x speed up we are having now.</p>

    </div>
  </div>


    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-68358910-2', 'auto');
      ga('send', 'pageview', 'slam');
	</script>

	<footer class="page-footer amanda-navbar">&nbsp;&nbsp;&nbsp;&nbsp;<br>
		<div class="footer-copyright">&nbsp;&nbsp;&nbsp;&nbsp;adriel luo. amanda eng. spring 2017.</div>
	</footer>
</body>
</html>