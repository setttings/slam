<!DOCTYPE html>
<html>
<head>
	<title>15418 Final Project</title>

	<!--Import Google Icon Font-->
	

	<meta name="viewport" content="width=device-width, initial-scale=1.0"/>

	<script type="text/javascript" src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
    <link href="http://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet">
  	<link rel="stylesheet" href="css/materialize.min.css">
    <link rel="stylesheet" href="css/style.css">
  	<script src="js/materialize.min.js"></script>
</script>

</head>

<body>
	<nav>
    <div class="nav-wrapper amanda-navbar">
      <a class="brand-logo">&nbsp;&nbsp;15-418 Final Project</a>
      <ul id="nav-mobile" class="right hide-on-med-and-down">
        <li><a href="proposal.html">Proposal</a></li>
        <li><a href="checkpoint.html">Checkpoint</a></li>
        <li><a href="index.html">Final Write Up</a></li>
        <li><a href="https://github.com/setttings/slam">Code</a></li>
      </ul>
    </div>
  </nav>

  <div id ="fullpage" class="container left-align">
    <div class="section">
     <h2>Parallel Nearest Neighbor Search for Velodyne LiDAR Point Clouds</h2>
     <p><a class="waves-effect waves-light btn" href="writeup.pdf">Writeup PDF</a> <a class="waves-effect waves-light btn" href="slides.pdf">Slides</a></p>
     <br>
     <h3>Final Write Up</h3>
     <p><a href="https://github.com/haoala">Adriel Luo</a> and <a href="https://github.com/setttings">Amanda Eng</a></p>
     <h5>Summary</h5>
     <p>We implemented a parallel nearest neighbor search that is optimized for point clouds returned by Velodyne LiDAR sensor, which are universally used on autonomous vehicles for sensing, localization and mapping. Nearest neighbor search is a fundamental step in iterative closest point (ICP) algorithms used to perform mapping. Our algorithm is achieves a 2x speed up over an optimized sequential kdtree-based nearest neighbors library implementation through GPU parallelism and by taking advantage of the 3D structure of Velodyne LiDAR point clouds.</p>
     <br>
     <h5>Background</h5>
     <p>Lidar sensors provide 3D information about the environment. The information they return is in the form of point clouds: sets of 3D points, each with an (x,y,z) coordinate. Lidar sensors shoot laser beams into the environment. The laser beams hit surfaces and return to the sensor. By measuring the time of flight for each beam, the sensor is able to know how far surfaces are in particular directions. Using many laser beams, a lidar sensor generates a point cloud that encodes the 3D structure of the environment.</p>
     <p>Lidar sensors are almost universally used on autonomous vehicle. In particular, the Velodyne lidar sensor is an extremely common sensor that is mounted on top of many self-driving cars. The point clouds it returns are useful for sensing, localization and mapping. Velodyne point clouds look like this:</p>
     <p>A common algorithm used for mapping is point cloud matching, where the difference between two point clouds is minimized. By overlapping point clouds returned sequentially from the lidar, a map is built, and the vehicle can localize itself in this map simultaneously.</p>
     <p>A key step in point cloud matching is nearest neighbor search. In the matching of two point clouds, each point in one point cloud looks for its nearest neighbor (by Euclidean distance) in the other point cloud. General nearest neighbor search is often implemented with a k-d tree, which gives an average O(log n) search time.</p>

     <br>

     <h5>Data Structures</h5>
     <h6><u>3D Point</u><h6>
     <p>typedef struct { float x; float x; float x;} Point; </p>

     <h6><u>Point Cloud</u><h6>
      <p>typedef std::vector<Point> PointCloud;</p>

      <br>
      <h5>Problem Statement</h5>
      <p>Input: PointCloud p0 with N0 points, PointCloud p1 with N1 points</p>
      <p>Output: int nearestNeighbor[N1] such that nearestNeighbor[i] is the index of p1[i]’s nearest neighbor in p0, i.e. p0[nearestNeighbor[i]] is the closest point in p0 to p1[i]</p>
      <p>Parallelizing this algorithm is not trivial due to hardware and memory constraints. Point clouds have more than 120,000 points on average, so a naive pairwise brute force distance computation would need enough memory for 120,000&sup2; = 14 billion entries, or 14 billion threads in total. Therefore we need to be smart about data reuse in order to reduce the memory footprint of the algorithm. We also try to use shared memory where possible to reduce the latency of memory access.</p>
      <br>

      <h5>Approach</h5>
      <h6><u>Establishing a baseline</u></h6>
      <p>We are trying to outperform a sequential kd-tree-based nearest neighbor search implementation that is fast and optimized (the Fast Library for Approximate Nearest Neighbors or FLANN), so the first step is to figure out how long that is taking.</p>
      <p>We took two timings: the time it took to insert the 120,000 points in the k-d tree (construction), and the time it took to query the k-d tree for the nearest neighbor for each of 120,000 points (querying).</p>
      <p>The overall timing we are looking beat for 120,000 queries into a cloud of 120,000 points is 0.229s.</p>

      <h6><u>Naive sequential implementation (brute-force)</u></h6>

      <h6><u>Deciding to use CUDA</u></h6>
      <p>The problem finding the nearest neighbor between a point from cloud1 and all points of cloud0 involves calculating the distance between a point and all others.  This will invoke the same set of instructions on different pair of points, making it amenable to parallelism with SIMD. This, combined with the facts that cloud0 contains thousands of points and that the GPU has hundreds of vector lanes for SIMD parallelism, makes this a good problem to be carried out with CUDA.</p>

      <h6><u>Parallelizing over points</u></h6>
      <p>Our first pass was a CUDA implementation that parallelized over the pixels in the reference point cloud and calculated the nearest neighbor for each point in the target point cloud sequentially.  We mapped each point in the reference point cloud to a CUDA thread.  For each point a in the query cloud, we launched a CUDA kernel.  Every thread in the kernel calculated the Euclidian between the coordinates of the point from the reference cloud it was assigned to, and the a. When this was done, we reduced across the threads within a threadblock to find the index of the point in cloud 0 that was nearest to a.  We then performed another reduction across the values returned from all threadblocks to find the minimum. </p>
      <p>Pseudocode for what we did is as follows.</p>

      <p>In order to find out if we are maximizing the parallelism available with CUDA, we varied the number of points in the reference point cloud and obtained the following graph. </p>

      <p>As we can see from Figures 1 and 2 above, the CUDA times fluctuated somewhat (and were actually slower than the sequential implementation)  when the reference dataset contained around 2,000 points. Figure 2 also indicated that the overheads of the CUDA kernel calls dominated the overall time taken, especially when there were fewer points.  </p>

      <p>When there was a larger number of points in cloud 0, there was a somewhat linear increase in time as the number of points in the dataset increased. This indicated that we were achieving the maximum parallelism capabilities of the machine given the constraints of our algorithm and dataset.  </p>

      <p>Despite the massive parallelism that CUDA brought, finding the nearest neighbors took 9.4s, 40.3x slower than the sequential PCL kd-tree implementation with FLANN for the same dataset. This led us to think of other implementations that reduced the number of times we launched a CUDA kernel.</p>

      <h6><u>Parallelizing over points in with more kernel</u></h6>
      <p>We changed tack and shifted the looping over query points into the kernel itself to minimize the number of kernel calls. The mapping of cloud 0 to threads remain unchanged. However, each threadblock is now computing the nearest neighbors for multiple values before returning. This took 2.0s, 4.3x faster our prior parallel CUDA attempt. This reduction in time took place because we were now reducing the number of CUDA kernel calls by a few orders of magnitude. </p>

      <h6><u>Reducing search space</u></h6>
      <p>After our first 2 tries were met with limited success, we came to the conclusion that we would have to do some prior processing of the point cloud to reduce the search space to have any hope of beating the optimized FLANN algorithm. The point clouds produced by lidar sensors have a very specific 3-D structure like the one below. This led to the idea of partitioning the reference into bins, and searching only in a particular bin for a point’s nearest neighbor.</p>

      <p>Reducing the search space for nearest neighbors gives us significant additional speed up. We found that 99% of the time, the nearest neighbor of a point would be within 2.5º of it in the azimuth. Taking advantage of the 3D structure of Velodyne point clouds, we can partition the point clouds by azimuth, and only search for nearest neighbors in the partition with similar azimuth. We sacrifice some correctness in reducing the search space because the correct nearest neighbor might not be in the partition we are searching, but for most applications having some outliers is acceptable to get a speedup.</p>

      <p>Depending on the level of correctness needed by an application and whether it is finding nearest neighbors in consecutive or non-consecutive frames, one can choose the maximum amount of speedup that meets the constraints.</p>

      <p>The pseudocode for this iteration of our code looked as follows:</p>

      <p>We noticed from our partitioning into of the points into 300 segments meant that each segment contained less than 512 points from cloud 0 in each bin.  Like Attempts 1 and 2, we assigned each point in cloud 0 to a CUDA thread and computed the distance from a point in cloud 1 to all these points, before doing a reduction to find the minimum.  However, since the sample space has been reduced, the computation needed to find the nearest neighbor for any point in cloud 1 has been localized to a single thread block.  This meant that there was no need to wait for all threadblocks to return doing a final reduction.  This eliminated a large portion of work done from our earlier implementations.</p>

      <br>
      <h5>Summary of Results</h5>
      <p>The timings and speedups that we obtained from the approaches mentioned above are summarized in the following table. Problem sizes were consistent across testing for all algorithms. Performance was measured according to the speedup w.r.t. the FLANN implementation. </p>

      <p>* The timings above were the averages of 10 trials taken on the same machine.</p>

      <p>One interesting thing we noticed was that although our final implementation did not beat the total time taken by FLANN, its query time was actually 1.69x faster than FLANN’s, and our overall speedup was limited due to the high overhead of constructing our data structures.  </p>

      <p>To go deeper into understanding why the time required was so much higher than FLANN’s, we broke down the construction time for our last approach into 2 sections: (1) Calculating which partition each point belong to and (2) Copying this data (1-dimensional array) into CUDA memory. We found that the latter took up 93% of the 0.246 second construction cost.  This problem could potentially be eliminated by the use of Unified Memory, which is something that we regretfully did not have the time to explore. </p>

      <p>We believe our choice of target machine was sound, but it will be interesting to see how runtimes will differ with ISPC and whether the high construction overheads could be avoided here.</p>

      <br>

      <h5>References</h5>
      <p><a href="http://www.pointclouds.org/">Points Cloud Library</a></p>
      <p><a href="http://velodynelidar.com/downloads.html">Point Cloud Datasets</a></p>
      <p><a href="https://en.wikipedia.org/wiki/K-d_tree">K-d trees</a></p>
      <p><a href="https://devblogs.nvidia.com/parallelforall/unified-memory-in-cuda-6/">Nvidia Blog on Unified Memory</a></p>
      <br>
      <h5>List of work done by each student</h5>
      <p>Equal work was performed by each student</p>


    </div>



    <br><br><br>


  	<div class="section">
  	 <h3>Write Up ----- Preliminary Draft</h3>
     <p><a href="https://github.com/haoala">Adriel Luo</a> and <a href="https://github.com/setttings">Amanda Eng</a></p>
     <h5>Summary</h5>
     <p>We implemented a parallel nearest neighbor search that is optimized for point clouds returned by Velodyne LiDAR sensor, which are universally used on autonomous vehicles for sensing, localization and mapping. Nearest neighbor search is a fundamental step in iterative closest point (ICP) algorithms used to perform mapping. Our algorithm is currently able to achieve a 10x speed up over an optimized sequential kdtree-based nearest neighbors library implementation through GPU parallelism and by taking advantage of the 3D structure of Velodyne LiDAR point clouds, but we expect that we will be able to achieve a speed up can be as high as 100x.</p>
     <br>
     <h5>Major technical challenges</h5>
     <p>One of the challenges arises from the fact that we are trying to outperform a sequential kd-tree-based nearest neighbor search implementation that is is fast and optimized (the Fast Library for Approximate Nearest Neighbors or FLANN). This means that we need to minimize the impact of the overhead of using the GPU for computation.</p>
     <p>Parallelizing this algorithm is not trivial due to hardware and memory constraints. Point clouds have more than 120,000 points on average, so a naive pairwise brute force distance computation would need enough memory for 120,000&sup2; = 14 billion entries, or 14 billion threads in total. Therefore we need to be smart about data reuse in order to reduce the memory footprint of the algorithm. We also try to use shared memory where possible to reduce the latency of memory access.</p>
     <br>
     <h5>Preliminary Results</h5>
     <h6><i>Establishing a baseline</i></h6>
     <p>Let the reference dataset refer to the points captured at time t, and the query dataset refer to data obtained at time t+1. For this problem, we intend to find the nearest neighbor in the reference dataset for every point in the query dataset.</p>
     <p>In the table below, the construction phase referred to the time it took to create a point cloud, and the querying phase refers to the total time taken for every point in the query dataset to find its nearest neighbor in the reference dataset. Both datasets contain more than 120,000 points.</p>
     <center><img src="img/img7.png" width="300" height="300" align="middle"></center>
     <br>

     <h6><i>Parallelizing over points</i></h6>
     <p>We developed a CUDA implementation for nearest neighbors that parallelized over the pixels in the reference point cloud and calculated the nearest neighbor for each point in the target point cloud sequentially.  In order to find out if we are maximizing the parallelism available with CUDA, we varied the number of points in the reference point cloud and obtained the following graph. </p>
     <center><img src="img/img3.png" width="600" height="400" align="middle"></center>
     <center><img src="img/img4.png" width="600" height="400" align="middle"></center>
     <p>As we can see from Figures 1 and 2 above, the CUDA times remained around the same (and were actually slower than the sequential implementation)  when the reference dataset contained around 2,000 points. This indicated that the overheads of the CUDA kernel calls dominated the time taken.</p>
     <p>Despite the massive parallelism that using CUDA brought, it was still <b>40.3x slower than the sequential PCL kd-tree implementation</b> with FLANN.  This led us to think of other implementations that involved launching fewer CUDA kernels or preprocessing the data.</p>
     <br>

     <h6><i>Parallelizing over points in batches</i></h6>
     <p>We moved on to use parallelize over points in batches of size <i>chunksize</i>.  This meant that the work performed by each CUDA thread increased by a factor of chunksize, but we made less CUDA calls by a factor of chunksize.  This turned out to be <b>4.3x faster our prior parallel CUDA algorithm and 10.17x slower than the sequential PCL kd-tree implementation</b>.  This speedup can possibly be explained by the overhead of launching the additional CUDA kernels. </p>
     <br>
     <h6><i>Reducing search space</i></h6>
     <center><img src="img/img6.png" width="500" height="300" align="middle"></center>
     <p>Reducing the search space for nearest neighbors gives us significant additional speed up. Taking advantage of the 3D structure of Velodyne point clouds, as well as the most common use case, we can partition the point clouds by azimuth, and only search for nearest neighbors in the partition with similar azimuth. We sacrifice some correctness in reducing the search space, because the correctness nearest neighbor might not be within the space we are searching, but because this is an optimization problem and the data has noise to begin with, having some outliers is acceptable.</p>

     <center><img src="img/img5.png" width="600" height="350" align="center"></center>

     <p>We see that correctness is maintained better for nearest neighbor searches between consecutive frames than between non-consecutive frames. This is not surprising, since consecutive frames, with a temporal difference of only 0.1s, are structurally similar and the simplifying assumptions we make above about where to find the nearest neighbor are more valid. Indeed, searching for nearest neighbors between consecutive frames is the most common use case, because in lidar mapping applications, the map is incrementally built every frame, and every new incoming frame is matched with the prior frame.</p>

     <p>The horizontal axis, speedup, is proportional to the number of radial partitions. We can sacrifice some correctness to achieve a significant speed up. For instance, <b>with consecutive frames, we can achieve a speedup of 48x and still find the correct nearest neighbor 99.7% of the time.</b> We can achieve <b>correctness scores of 98.2% and 94.9% with a speedup of 100x and 200x respectively</b>.</p>

     <p>Depending on the level of correctness needed by an application and whether it is finding nearest neighbors in consecutive or non-consecutive frames, one can choose the maximum amount of speedup that meets the constraints.</p>
     <br>

     <h5>Summary of what final numbers on Friday are expected to show</h5>
     <p>We expect that, once we are able to parallelize over points in the reference and target point clouds, we will be able to exceed the 10x speed up we are having now.</p>

    </div>
  </div>


    <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-68358910-2', 'auto');
      ga('send', 'pageview', 'slam');
	</script>

	<footer class="page-footer amanda-navbar">&nbsp;&nbsp;&nbsp;&nbsp;<br>
	</footer>
</body>
</html>